{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-Backour/6.4610-psets/blob/main/hw1_nlp_shared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CD98gW5NbOVA",
        "outputId": "1dd2362b-3580-4d58-aa65-db91f1fc94c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nHomework 1: Language Models\\n\\nThis assignment will guide you through implementing three different types of language models:\\n1. N-gram model\\n2. Log-linear model with hand-designed features\\n3. Continuous Bag of Words (CBOW) model\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Homework 1: Language Models\n",
        "\n",
        "This assignment will guide you through implementing three different types of language models:\n",
        "1. N-gram model\n",
        "2. Log-linear model with hand-designed features\n",
        "3. Continuous Bag of Words (CBOW) model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a5ooSqa9bTXF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Optional, Tuple\n",
        "import pickle\n",
        "from collections import defaultdict, Counter\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.models import BPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ywr2egPPbVQK"
      },
      "outputs": [],
      "source": [
        "class LanguageModel:\n",
        "    \"\"\"\n",
        "    Abstract base class for language models.\n",
        "\n",
        "    This class defines the interface that all language models should implement.\n",
        "    Language models take a sequence of tokens (represented as integers) and\n",
        "    predict probability distributions over the next token in the vocabulary.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.eos = 'eos'\n",
        "\n",
        "    def train(self, token_sequences: List[np.ndarray]) -> None:\n",
        "        \"\"\"\n",
        "        Train the language model on a collection of token sequences.\n",
        "\n",
        "        Args:\n",
        "            token_sequences (List[np.ndarray]): List of token sequences, where each\n",
        "                                               sequence is a numpy array of integers\n",
        "                                               representing token IDs.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement the train method\")\n",
        "\n",
        "    def get_next_token_probs(self, context: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get probability distribution over next tokens given a context.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Array of token IDs representing the context.\n",
        "                                 Shape: (context_length,)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Probability distribution over vocabulary.\n",
        "                       Shape: (vocab_size,)\n",
        "                       Should sum to 1.0.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement the get_next_token_probs method\")\n",
        "\n",
        "    def perplexity(self, token_sequences: List[np.ndarray]) -> float:\n",
        "        \"\"\"\n",
        "        Calculate perplexity of the model on a set of token sequences.\n",
        "\n",
        "        Perplexity is 2^(-average_log_likelihood), where average log likelihood\n",
        "        is calculated over all tokens in all sequences.\n",
        "\n",
        "        Args:\n",
        "            token_sequences (List[np.ndarray]): List of token sequences to evaluate\n",
        "\n",
        "        Returns:\n",
        "            float: Perplexity score (lower is better)\n",
        "        \"\"\"\n",
        "        total_log_likelihood = 0.0\n",
        "        total_tokens = 0\n",
        "\n",
        "        for sequence in token_sequences:\n",
        "            if len(sequence) < 2:\n",
        "                continue  # Need at least 2 tokens (context + target)\n",
        "\n",
        "            for i in range(1, len(sequence)):\n",
        "                context = sequence[:i]\n",
        "                target_token = sequence[i]\n",
        "\n",
        "                probs = self.get_next_token_probs(context)\n",
        "                # Add small epsilon to avoid log(0)\n",
        "                prob = max(float(probs[target_token]), 1e-10)\n",
        "                total_log_likelihood += np.log2(prob)\n",
        "                total_tokens += 1\n",
        "\n",
        "        if total_tokens == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        average_log_likelihood = total_log_likelihood / total_tokens\n",
        "        return 2 ** (-average_log_likelihood)\n",
        "\n",
        "    def generate_text(self, context: np.ndarray, max_length: int = 100,\n",
        "                     temperature: float = 1.0) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate text by sampling from the model.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Initial context tokens\n",
        "            max_length (int): Maximum number of tokens to generate\n",
        "            temperature (float): Sampling temperature (higher = more random)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Generated sequence including the initial context\n",
        "        \"\"\"\n",
        "        generated = list(context)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            current_context = np.array(generated)\n",
        "            probs = self.get_next_token_probs(current_context)\n",
        "\n",
        "            # Apply temperature\n",
        "            if temperature != 1.0:\n",
        "                probs = np.power(probs, 1.0 / temperature)\n",
        "                probs = probs / np.sum(probs)\n",
        "\n",
        "            # Sample next token\n",
        "            next_token = np.random.choice(len(probs), p=probs)\n",
        "            generated.append(next_token)\n",
        "\n",
        "            # Optional: add stopping criteria here (e.g., end-of-sequence token)\n",
        "            if next_token == self.eos:\n",
        "              break\n",
        "\n",
        "        return np.array(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "qqLcUlo4bY4-"
      },
      "outputs": [],
      "source": [
        "class NGramModel(LanguageModel):\n",
        "    \"\"\"\n",
        "    N-gram language model using maximum likelihood estimation with smoothing.\n",
        "\n",
        "    This model predicts the next token based on the previous n-1 tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, n: int = 3, smoothing=None):\n",
        "        \"\"\"\n",
        "        Initialize the N-gram model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            n (int): Order of the n-gram (default: 3 for trigram)\n",
        "\n",
        "            Bonus! Without smoothing your perplexity will be pretty bad. If you\n",
        "            implement some kind of smoothing and get the perplexity below 300\n",
        "            you'll get extra credit.\n",
        "            smoothing (str): Smoothing method ('laplace' or 'interpolation')\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Hint: Consider using nested dictionaries or defaultdict(Counter) to store counts.\n",
        "        # Hint: Consider how you will handle different context lengths.\n",
        "        # At the start of a sentence, you might have 0, 1, or 2 words of context\n",
        "        # instead of the full n-1 words.\n",
        "        self.n = n\n",
        "        self.vocab_size = vocab_size\n",
        "        self.counts = {}\n",
        "    def train(self, token_sequences: List[np.ndarray]) -> None:\n",
        "        \"\"\"\n",
        "        Train the n-gram model by counting n-grams in the training data.\n",
        "\n",
        "        Args:\n",
        "            token_sequences (List[np.ndarray]): Training sequences\n",
        "        \"\"\"\n",
        "        for doc in token_sequences:\n",
        "            for i in range(len(doc) - self.n + 1):\n",
        "                for j in range(self.n):\n",
        "                    window = tuple(doc[i:i+j+1])\n",
        "                    self.counts.setdefault(window,0)\n",
        "                    self.counts[window] +=1\n",
        "\n",
        "\n",
        "    def get_next_token_probs(self, context: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get probability distribution over next tokens for given context.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Context tokens\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Probability distribution over vocabulary\n",
        "        \"\"\"\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Hint: Try contexts from longest to shortest, i.e., try the full context,\n",
        "        # and if it is not in the training data, try shorter context\n",
        "        # Hint: What probability distribution should we output if no valid context is found?\n",
        "        for i in range(self.n - 1,0 ,-1):\n",
        "            window = tuple(context[-i:])\n",
        "            if window in self.counts:\n",
        "                prb =  np.array([self.counts.get(window + (new_word,),0)/self.counts[window] for new_word in range(self.vocab_size)])\n",
        "                return prb/sum(prb)\n",
        "        prb =  np.array([self.counts.get(new_word,0) for new_word in range(self.vocab_size)])\n",
        "        if sum(prb):\n",
        "            return prb/sum(prb)\n",
        "        return np.array([1/self.vocab_size for _ in range(self.vocab_size)])\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Celb4A0sbkAN"
      },
      "outputs": [],
      "source": [
        "class LogLinearModel(LanguageModel):\n",
        "    \"\"\"\n",
        "    Log-linear language model with hand-designed features.\n",
        "\n",
        "    This model uses a linear combination of features to predict next token probabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, context_size: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the log-linear model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            context_size (int): Number of context tokens to consider\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: YOUR CODE HERE\n",
        "\n",
        "        self.context_size = context_size\n",
        "        self.vocab_size = vocab_size\n",
        "        # Hint: You may find the class nn.Linear useful\n",
        "        # If this is too slow or using too much memory, check out the nn.EmbeddingBag class\n",
        "        # and see if that's applicable to your use case\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(context_size * vocab_size,vocab_size)\n",
        "            )\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01)\n",
        "\n",
        "    def extract_features(self, context: np.ndarray) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Extract features from the context.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Context tokens\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Feature vector produced from context tokens\n",
        "        \"\"\"\n",
        "        features = torch.zeros(self.vocab_size,self.context_size,dtype=torch.float32)\n",
        "        relevant_context = context[-self.context_size:]\n",
        "        for i,word in enumerate(relevant_context):\n",
        "            features[int(word),i] = 1.0\n",
        "        return features.view(-1)\n",
        "\n",
        "    def train(self, token_sequences: List[np.ndarray], epochs: int = 2, batch_size: int = 32) -> None:\n",
        "        \"\"\"\n",
        "        Train the log-linear model using gradient descent.\n",
        "        \"\"\"\n",
        "        # Create training examples (context, target) pairs\n",
        "        contexts_list = []\n",
        "        targets_list = []\n",
        "        for seq in token_sequences:\n",
        "            contexts_list.extend(seq[i:i+self.context_size] for i in range(len(seq) - self.context_size))\n",
        "            targets_list.extend(seq[self.context_size:])\n",
        "        assert len(contexts_list) == len(targets_list)\n",
        "        all_features = contexts_list\n",
        "        # TODO: YOUR CODE HERE\n",
        "\n",
        "\n",
        "        print(f\"Training on {len(all_features)} examples for {epochs} epochs...\")\n",
        "\n",
        "        # Training loop\n",
        "        # TODO: Put your layers in training mode\n",
        "        self.model.train()\n",
        "        losses = []  # Potentially useful for debugging (loss should go down!)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(device)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch}\")\n",
        "            total_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            # Mini-batch training\n",
        "            # Note: tqdm is used to display progress bars for loops, helping visualize training progress.\n",
        "            for i in tqdm(range(0, len(all_features), batch_size)):\n",
        "                batch_contexts= contexts_list[i:i+batch_size]\n",
        "                batch_targets = targets_list[i:i+batch_size]\n",
        "\n",
        "                # TODO: Get features for the batch\n",
        "                batch_contexts = torch.stack([self.extract_features(c) for c in batch_contexts])\n",
        "                batch_contexts = batch_contexts.to(device)\n",
        "                batch_targets = torch.tensor(batch_targets,device=device,dtype=torch.int64)\n",
        "                # TODO: Zero the gradients of the optimizer\n",
        "                \n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "                # TODO: Perform a forward pass to compute predictions for the model.\n",
        "                logits = self.model(batch_contexts)\n",
        "\n",
        "\n",
        "                # TODO: Perform the backward pass and gradient update. Remember,\n",
        "                # you need to compute the loss, perform the backward pass, and\n",
        "                # update the model parameters.\n",
        "                # Your code here!\n",
        "                loss = self.criterion(logits,batch_targets)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "                if i % (batch_size * 10) == 0:  # Print every 10 batches\n",
        "                    print(f\"Epoch {epoch}, Batch {i // batch_size}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "            if epoch % 2 == 0:\n",
        "                avg_loss = total_loss / num_batches\n",
        "                print(f\"Epoch {epoch}: Average Loss = {avg_loss:.4f}\")\n",
        "\n",
        "            losses.append(total_loss)\n",
        "        torch.save(self.model.state_dict(),'log_linear.pth')\n",
        "\n",
        "    def get_next_token_probs(self, context: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get probability distribution using softmax over linear scores.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Context tokens\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Probability distribution over vocabulary\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            if len(context) == 0:\n",
        "                return torch.full((self.vocab_size,),1.0/self.vocab_size)\n",
        "            features = self.extract_features(context).unsqueeze(0)\n",
        "            features = features.to(next(self.model.parameters()).device)\n",
        "            logits = self.model(features)\n",
        "            prb = torch.softmax(logits, dim=-1).cpu().numpy().flatten()\n",
        "        return prb\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Hint: What probability distribution should we output if no valid context is found?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT1D2snWcHSy"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "class CBOWModel(LanguageModel):\n",
        "    \"\"\"\n",
        "    Continuous Bag of Words (CBOW) model.\n",
        "\n",
        "    This model learns dense vector representations of words and predicts\n",
        "    the next word from the context words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int = 100, context_size: int = 2, learning_rate: float = 0.01):\n",
        "        \"\"\"\n",
        "        Initialize the CBOW model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            embedding_dim (int): Dimension of word embeddings\n",
        "            context_size (int): Number of context words on each side\n",
        "            learning_rate (float): Learning rate for training\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # You may find the classes nn.Embedding and nn.EmbeddingBag useful\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.context_size = context_size\n",
        "        self.w = nn.Embedding(vocab_size,embedding_dim)\n",
        "        self.u = nn.Embedding(vocab_size,embedding_dim)\n",
        "        # We use an Adam optimizer. This is a fancy version of SGD which uses momentum and adaptive updates.\n",
        "        self.optimizer = optim.Adam(itertools.chain(self.w.parameters(), self.u.parameters()), lr=learning_rate)\n",
        "\n",
        "        # What loss function should we use for Word2Vec?\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    def train(self, token_sequences: List[np.ndarray], epochs: int = 10, batch_size: int = 32) -> None:\n",
        "        \"\"\"\n",
        "        Train the CBOW model.\n",
        "\n",
        "        Args:\n",
        "            token_sequences (List[np.ndarray]): Training sequences\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "        \"\"\"\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Create training examples (context, target) pairs\n",
        "        # Hint, extract left context only for next token prediction\n",
        "        # Hint, pad shorter contexts with 0, this ensures all have the same length for batching\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        contexts_list = []\n",
        "        targets_list = []\n",
        "        for seq in token_sequences:\n",
        "            contexts_list.extend(np.concatenate((seq[i - self.context_size : i],seq[i+1:i + self.context_size + 1])) for i in range(self.context_size, len(seq)-self.context_size - 1))\n",
        "            targets_list.extend([seq[i] for i in range(self.context_size, len(seq)-self.context_size - 1)])\n",
        "        assert len(contexts_list) == len(targets_list)\n",
        "        all_contexts = torch.tensor(contexts_list).to(device=device)\n",
        "        all_targets = torch.tensor(targets_list).to(device=device)\n",
        "        print(f\"the shape of contexts is {all_contexts.shape}\")\n",
        "        print(f\"the shape of targets is {all_targets.shape}\")\n",
        "\n",
        "        # TODO: Put your layers in training mode\n",
        "        self.w.train()\n",
        "        self.u.train()\n",
        "        self.u.to(device=device)\n",
        "        self.w.to(device=device)\n",
        "        losses = []  # Potentially useful for debugging (loss should go down!)\n",
        "        # Note: tqdm is used to display progress bars for loops, helping visualize training progress.\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch}\")\n",
        "            total_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            # Shuffle data\n",
        "            indices = torch.randperm(len(all_contexts))\n",
        "            all_contexts = all_contexts[indices]\n",
        "            all_targets = all_targets[indices]\n",
        "\n",
        "            for i in tqdm(range(0, len(all_contexts), batch_size)):\n",
        "                # As an alternative to this implementation, you can experiment with\n",
        "                # DataLoader (https://docs.pytorch.org/docs/stable/data.html) for automatic shuffling, parallel loading\n",
        "                batch_contexts = all_contexts[i:i+batch_size]\n",
        "                batch_targets = all_targets[i:i+batch_size]\n",
        "\n",
        "                # TODO: Zero the gradients of the optimizer\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # TODO: Perform a forward pass to compute predictions for the model.\n",
        "                # Your code here!\n",
        "\n",
        "                preds = torch.mean(self.w(batch_contexts),dim=1) @ self.u.weight.T\n",
        "\n",
        "                # TODO: Finish the backward pass and gradient update.\n",
        "                # Remember, you need to compute the loss, perform the backward pass, and\n",
        "                # update the model parameters.\n",
        "                # Your code here!\n",
        "                loss = self.criterion(preds,batch_targets)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "                if i % (batch_size * 10) == 0:  # Print every 10 batches\n",
        "                    print(f\"Epoch {epoch}, Batch {i // batch_size}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "            if epoch % 2 == 0:\n",
        "                avg_loss = total_loss / num_batches\n",
        "                print(f\"Epoch {epoch}: Average Loss = {avg_loss:.4f}\")\n",
        "\n",
        "            losses.append(total_loss)\n",
        "\n",
        "    def get_next_token_probs(self, context: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get next-token probability distributions.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Context tokens\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Probability distribution over vocabulary\n",
        "        \"\"\"\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Hints:\n",
        "        # For next-token prediction, we use the last context_size tokens\n",
        "        # No valid context, return uniform distribution\n",
        "        # Pad context to expected size\n",
        "        # Don't forget to add batch dimension, torch expects (batch_size, context_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if len(context) == 0:\n",
        "                return torch.full((self.vocab_size,),1.0/self.vocab_size)\n",
        "            padded_context = np.pad(context,(self.context_size,0),mode = 'constant')\n",
        "            relevant = torch.tensor(padded_context[-self.context_size:]).to(device=next(self.u.parameters().device()))\n",
        "            w = self.w(relevant).unsqueeze(0)\n",
        "            w_avg = torch.mean(w,dim = 1)\n",
        "            logits = w_avg @ self.u.T\n",
        "            return torch.softmax(logits,dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "\n",
        "    def get_word_embedding(self, token_id: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get the learned embedding for a specific token.\n",
        "\n",
        "        Args:\n",
        "            token_id (int): Token ID\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Word embedding vector\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.w(token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_3fWkVnxcXu4"
      },
      "outputs": [],
      "source": [
        "def load_data(filepath: str, tokenizer: Optional[Tokenizer], max_seq_length: int = 512) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load and preprocess text data using GPT-2 tokenizer.\n",
        "\n",
        "    This function is provided complete - students don't need to modify it.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the text file\n",
        "        tokenizer (Optional[Tokenizer]): Tokenizer to use. If None, a new tokenizer will be created.\n",
        "        max_seq_length (int): Maximum sequence length for splitting text\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[np.ndarray], GPT2Tokenizer]: List of token sequences and the tokenizer\n",
        "    \"\"\"\n",
        "    # Byte Pair Encoding (BPE)\n",
        "\n",
        "    if tokenizer is None:\n",
        "        tokenizer = Tokenizer(BPE())\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        #trainer = BpeTrainer(special_tokens=[\"[PAD]\"])\n",
        "        tokenizer.train([filepath])\n",
        "\n",
        "    # Read the text file\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Tokenize the entire text\n",
        "    tokens = tokenizer.encode(text).ids\n",
        "\n",
        "    # Split into sequences of max_seq_length\n",
        "    sequences = []\n",
        "    for i in range(0, len(tokens), max_seq_length):\n",
        "        sequence = tokens[i:i+max_seq_length]\n",
        "        if len(sequence) > 1:  # Need at least 2 tokens for language modeling\n",
        "            sequences.append(np.array(sequence))\n",
        "\n",
        "    print(f\"Loaded {len(sequences)} sequences from {filepath}\")\n",
        "    print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
        "    print(f\"Sample tokens: {tokens[:10]}\")\n",
        "    print(f\"Sample text: {tokenizer.decode(tokens[:10])}\")\n",
        "\n",
        "    return sequences, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0ItN7U7ocbkA"
      },
      "outputs": [],
      "source": [
        "def evaluate_models(models: List[LanguageModel], test_data: List[np.ndarray]) -> None:\n",
        "    \"\"\"\n",
        "    Evaluate and compare multiple language models.\n",
        "\n",
        "    Args:\n",
        "        models (List[LanguageModel]): List of trained models\n",
        "        test_data (List[np.ndarray]): Test sequences\n",
        "    \"\"\"\n",
        "    print(\"Model Evaluation Results:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        model_name = model.__class__.__name__\n",
        "        try:\n",
        "            ppl = model.perplexity(test_data)\n",
        "            print(f\"{model_name}: Perplexity = {ppl:.2f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"{model_name}: Error calculating perplexity - {e}\")\n",
        "\n",
        "    print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xMtmXsBVvcWX"
      },
      "outputs": [],
      "source": [
        "def analyze(model1: LanguageModel, model2: LanguageModel, test_data: List[np.ndarray],\n",
        "           tokenizer=None, context_length: int = 2) -> dict:\n",
        "    \"\"\"\n",
        "    Compare two models and find contexts where each performs better.\n",
        "\n",
        "    Args:\n",
        "        model1 (LanguageModel): First model to compare\n",
        "        model2 (LanguageModel): Second model to compare\n",
        "        test_data (List[np.ndarray]): Test sequences\n",
        "        tokenizer: Tokenizer for decoding (optional, for display purposes)\n",
        "        context_length (int): Context length to consider\n",
        "\n",
        "    Returns:\n",
        "        dict: Analysis results including overall perplexities and context comparisons\n",
        "    \"\"\"\n",
        "    print(\"Detailed Model Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Overall perplexity comparison\n",
        "    try:\n",
        "        ppl1 = model1.perplexity(test_data)\n",
        "        ppl2 = model2.perplexity(test_data)\n",
        "\n",
        "        model1_name = model1.__class__.__name__\n",
        "        model2_name = model2.__class__.__name__\n",
        "\n",
        "        print(f\"{model1_name} overall perplexity: {ppl1:.3f}\")\n",
        "        print(f\"{model2_name} overall perplexity: {ppl2:.3f}\")\n",
        "        print(f\"Better overall model: {model1_name if ppl1 < ppl2 else model2_name}\")\n",
        "        print()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating overall perplexity: {e}\")\n",
        "        return {}\n",
        "\n",
        "    # Context-level analysis\n",
        "    context_comparisons = []\n",
        "    model1_better_contexts = []\n",
        "    model2_better_contexts = []\n",
        "\n",
        "    print(\"Analyzing context-level performance...\")\n",
        "\n",
        "    for seq_idx, sequence in enumerate(test_data):\n",
        "        for i in range(context_length, len(sequence)):\n",
        "            context = sequence[i - context_length:i]\n",
        "            target_token = sequence[i]\n",
        "\n",
        "            try:\n",
        "                # Get predictions from both models\n",
        "                probs1 = model1.get_next_token_probs(context)\n",
        "                probs2 = model2.get_next_token_probs(context)\n",
        "\n",
        "                # Calculate log probabilities for the actual target\n",
        "                prob1 = max(probs1[target_token], 1e-10)\n",
        "                prob2 = max(probs2[target_token], 1e-10)\n",
        "\n",
        "                log_prob1 = np.log2(prob1)\n",
        "                log_prob2 = np.log2(prob2)\n",
        "\n",
        "                # Store comparison data\n",
        "                context_info = {\n",
        "                    'context': context.copy(),\n",
        "                    'target': target_token,\n",
        "                    'log_prob1': log_prob1,\n",
        "                    'log_prob2': log_prob2,\n",
        "                    'seq_idx': seq_idx,\n",
        "                    'pos': i\n",
        "                }\n",
        "                context_comparisons.append(context_info)\n",
        "\n",
        "                # Categorize based on which model is better\n",
        "                if log_prob1 > log_prob2:  # Higher log prob = better\n",
        "                    model1_better_contexts.append(context_info)\n",
        "                else:\n",
        "                    model2_better_contexts.append(context_info)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error analyzing context at seq {seq_idx}, pos {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Calculate statistics\n",
        "    total_contexts = len(context_comparisons)\n",
        "    model1_wins = len(model1_better_contexts)\n",
        "    model2_wins = len(model2_better_contexts)\n",
        "\n",
        "    print(f\"Total contexts analyzed: {total_contexts}\")\n",
        "    print(f\"{model1_name} better contexts: {model1_wins} ({100*model1_wins/total_contexts:.1f}%)\")\n",
        "    print(f\"{model2_name} better contexts: {model2_wins} ({100*model2_wins/total_contexts:.1f}%)\")\n",
        "    print()\n",
        "\n",
        "    # Find patterns in contexts where each model excels\n",
        "    def analyze_context_patterns(better_contexts, model_name, top_k=10):\n",
        "        print(f\"Top {top_k} unique contexts where {model_name} excels:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Group contexts by (context, target) pairs\n",
        "        context_groups = {}\n",
        "        for ctx_info in better_contexts:\n",
        "            context_tuple = tuple(ctx_info['context'])\n",
        "            target = ctx_info['target']\n",
        "            key = (context_tuple, target)\n",
        "\n",
        "            if key not in context_groups:\n",
        "                context_groups[key] = {\n",
        "                    'contexts': [],\n",
        "                    'best_diff': 0,\n",
        "                    'context': ctx_info['context'],\n",
        "                    'target': target\n",
        "                }\n",
        "\n",
        "            context_groups[key]['contexts'].append(ctx_info)\n",
        "\n",
        "            # Track the best performance difference for this context\n",
        "            diff = (ctx_info['log_prob1'] - ctx_info['log_prob2']\n",
        "                   if model_name == model1_name\n",
        "                   else ctx_info['log_prob2'] - ctx_info['log_prob1'])\n",
        "\n",
        "            if diff > context_groups[key]['best_diff']:\n",
        "                context_groups[key]['best_diff'] = diff\n",
        "\n",
        "        # Sort by best performance difference\n",
        "        sorted_groups = sorted(context_groups.values(),\n",
        "                             key=lambda x: x['best_diff'],\n",
        "                             reverse=True)\n",
        "\n",
        "        for i, group in enumerate(sorted_groups[:top_k]):\n",
        "            context = group['context']\n",
        "            target = group['target']\n",
        "            count = len(group['contexts'])\n",
        "            best_diff = group['best_diff']\n",
        "\n",
        "            # Format context display\n",
        "            if tokenizer is not None:\n",
        "                try:\n",
        "                    context_text = tokenizer.decode(context[-min(5, len(context)):])\n",
        "                    target_text = tokenizer.decode([target])\n",
        "                    print(f\"{i+1:2d}. Context: '{context_text}' → Target: '{target_text}' (×{count})\")\n",
        "                except:\n",
        "                    print(f\"{i+1:2d}. Context: {context[-min(5, len(context)):]} → Target: {target} (×{count})\")\n",
        "            else:\n",
        "                print(f\"{i+1:2d}. Context: {context[-min(5, len(context)):]} → Target: {target} (×{count})\")\n",
        "\n",
        "            # Show best probability difference for this context type\n",
        "            print(f\"     Best log-prob difference: {best_diff:.3f}\")\n",
        "\n",
        "            # If there are multiple instances, show average difference\n",
        "            if count > 1:\n",
        "                avg_diff = sum((ctx['log_prob1'] - ctx['log_prob2']\n",
        "                              if model_name == model1_name\n",
        "                              else ctx['log_prob2'] - ctx['log_prob1'])\n",
        "                             for ctx in group['contexts']) / count\n",
        "                print(f\"     Average log-prob difference: {avg_diff:.3f}\")\n",
        "            print()\n",
        "\n",
        "    # Analyze patterns for both models\n",
        "    if model1_better_contexts:\n",
        "        analyze_context_patterns(model1_better_contexts, model1_name)\n",
        "\n",
        "    if model2_better_contexts:\n",
        "        analyze_context_patterns(model2_better_contexts, model2_name)\n",
        "\n",
        "    # Analyze context length effects\n",
        "    print(\"Performance by context length:\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    context_length_stats = {}\n",
        "    for ctx_info in context_comparisons:\n",
        "        ctx_len = len(ctx_info['context'])\n",
        "        if ctx_len not in context_length_stats:\n",
        "            context_length_stats[ctx_len] = {'model1_wins': 0, 'model2_wins': 0, 'total': 0}\n",
        "\n",
        "        context_length_stats[ctx_len]['total'] += 1\n",
        "        if ctx_info['log_prob1'] > ctx_info['log_prob2']:\n",
        "            context_length_stats[ctx_len]['model1_wins'] += 1\n",
        "        else:\n",
        "            context_length_stats[ctx_len]['model2_wins'] += 1\n",
        "\n",
        "    for ctx_len in sorted(context_length_stats.keys()):\n",
        "        stats = context_length_stats[ctx_len]\n",
        "        model1_pct = 100 * stats['model1_wins'] / stats['total']\n",
        "        model2_pct = 100 * stats['model2_wins'] / stats['total']\n",
        "        print(f\"Length {ctx_len:2d}: {model1_name} {model1_pct:5.1f}% | {model2_name} {model2_pct:5.1f}% ({stats['total']} examples)\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Return structured results\n",
        "    return {\n",
        "        'overall_perplexity': {model1_name: ppl1, model2_name: ppl2},\n",
        "        'context_level': {\n",
        "            'total_contexts': total_contexts,\n",
        "            f'{model1_name}_wins': model1_wins,\n",
        "            f'{model2_name}_wins': model2_wins,\n",
        "            f'{model1_name}_better_contexts': model1_better_contexts[:10],  # Top 10\n",
        "            f'{model2_name}_better_contexts': model2_better_contexts[:10],  # Top 10\n",
        "        },\n",
        "        'context_length_stats': context_length_stats\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "MNOveioKceQH",
        "outputId": "329b849f-11e4-4e02-f97e-80eb9c3edad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Loaded 3892 sequences from train.txt\n",
            "Vocabulary size: 13672\n",
            "Sample tokens: [774, 8, 774, 206, 88, 741, 239, 91, 137, 6]\n",
            "Sample text: Spot . Spot saw the shiny car and said ,\n",
            "Loaded training data\n",
            "Loaded 780 sequences from test.txt\n",
            "Vocabulary size: 13672\n",
            "Sample tokens: [1, 2313, 221, 166, 293, 205, 288, 8, 1, 1221]\n",
            "Sample text: \" Who are you ?\" Tim asked . \" Why\n",
            "Loaded test data\n",
            "N-gram\n",
            "\" Okay , mom .\" Their parents were very important that Mrs . Lung smiled again because he couldn ' t be selfish . She also put some magic to hide behind a tree , she found some yummy food on it . To the kitten were best friends . He only hears his tummy . She loved to jump so much fun . He was three years old and she couldn ' t believe his luck never ran out of the hill he became a good idea and they were never miserable again . The new flower in the pond\n",
            "Perplexity: 692.5371509498468\n"
          ]
        }
      ],
      "source": [
        "# Feel free to comment out portions of the code and run it multiple times, or to\n",
        "# take it out of the main() function. If you're struggling to lower your\n",
        "# perplexity, you can play around with the model hyperparameters like the\n",
        "# learning rate, batch size, and number of epochs.\n",
        "\n",
        "def main():\n",
        "    # Here we're training on train.txt and evaluating on test.txt.\n",
        "    # However, you might find it useful to play with tiny.txt while you're debugging.\n",
        "    # If you're running into memory issues, you can try training on a smaller set of\n",
        "    # sentences by truncating train.txt, but you should always report your final\n",
        "    # results on test.txt (and think about ways of making your code more efficient)!\n",
        "\n",
        "    train_data, tokenizer = load_data(\"train.txt\", tokenizer=None) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded training data\")\n",
        "    test_data, _ = load_data(\"test.txt\", tokenizer=tokenizer) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded test data\")\n",
        "\n",
        "    print('N-gram')\n",
        "    ngram_model = NGramModel(tokenizer.get_vocab_size(), n=3)\n",
        "    ngram_model.train(train_data)\n",
        "    print(tokenizer.decode(ngram_model.generate_text(test_data[0][:1])))\n",
        "    print(f\"Perplexity: {ngram_model.perplexity(test_data)}\")\n",
        "\n",
        "    # print('Log-linear')\n",
        "    # log_linear_model = LogLinearModel(tokenizer.get_vocab_size())\n",
        "    # log_linear_model.train(train_data)\n",
        "    # print(tokenizer.decode(log_linear_model.generate_text(test_data[0][:1])))\n",
        "    # print(f\"Perplexity: {log_linear_model.perplexity(test_data)}\")\n",
        "\n",
        "    # print('CBOW')\n",
        "    # cbow_model = CBOWModel(tokenizer.get_vocab_size(), embedding_dim=100, context_size=3)\n",
        "    # cbow_model.train(train_data, epochs=10)\n",
        "    # print(tokenizer.decode(cbow_model.generate_text(test_data[0][:1])))\n",
        "    # print(f\"Perplexity: {cbow_model.perplexity(test_data)}\")\n",
        "\n",
        "    # evaluate_models([ngram_model, log_linear_model,cbow_model], test_data)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Loaded 3892 sequences from train.txt\n",
            "Vocabulary size: 13672\n",
            "Sample tokens: [774, 8, 774, 206, 88, 741, 239, 91, 137, 6]\n",
            "Sample text: Spot . Spot saw the shiny car and said ,\n",
            "Loaded training data\n",
            "Loaded 780 sequences from test.txt\n",
            "Vocabulary size: 13672\n",
            "Sample tokens: [1, 2313, 221, 166, 293, 205, 288, 8, 1, 1221]\n",
            "Sample text: \" Who are you ?\" Tim asked . \" Why\n",
            "Loaded test data\n",
            "Log-linear\n",
            "Training on 1980603 examples for 2 epochs...\n",
            "Epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/61894 [00:07<123:55:01,  7.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 0: Loss = 9.5231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 7/61894 [00:45<112:39:40,  6.55s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[45], line 30\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# print('CBOW')\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# cbow_model = CBOWModel(tokenizer.get_vocab_size(), embedding_dim=100, context_size=3)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# cbow_model.train(train_data, epochs=10)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# evaluate_models([ngram_model, log_linear_model,cbow_model], test_data)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[45], line 16\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLog-linear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m log_linear_model \u001b[38;5;241m=\u001b[39m LogLinearModel(tokenizer\u001b[38;5;241m.\u001b[39mget_vocab_size())\n\u001b[0;32m---> 16\u001b[0m \u001b[43mlog_linear_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(log_linear_model\u001b[38;5;241m.\u001b[39mgenerate_text(test_data[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m1\u001b[39m])))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_linear_model\u001b[38;5;241m.\u001b[39mperplexity(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[44], line 99\u001b[0m, in \u001b[0;36mLogLinearModel.train\u001b[0;34m(self, token_sequences, epochs, batch_size)\u001b[0m\n\u001b[1;32m     97\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(logits,batch_targets)\n\u001b[1;32m     98\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    101\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/adam.py:478\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 478\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# LOG LINEAR MODEL\n",
        "\n",
        "def main():\n",
        "    # Here we're training on train.txt and evaluating on test.txt.\n",
        "    # However, you might find it useful to play with tiny.txt while you're debugging.\n",
        "    # If you're running into memory issues, you can try training on a smaller set of\n",
        "    # sentences by truncating train.txt, but you should always report your final\n",
        "    # results on test.txt (and think about ways of making your code more efficient)!\n",
        "    train_data, tokenizer = load_data(\"train.txt\", tokenizer=None) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded training data\")\n",
        "    test_data, _ = load_data(\"test.txt\", tokenizer=tokenizer) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded test data\")\n",
        "\n",
        "    print('Log-linear')\n",
        "    log_linear_model = LogLinearModel(tokenizer.get_vocab_size())\n",
        "    log_linear_model.train(train_data)\n",
        "    print(tokenizer.decode(log_linear_model.generate_text(test_data[0][:1])))\n",
        "    print(f\"Perplexity: {log_linear_model.perplexity(test_data)}\")\n",
        "\n",
        "    # evaluate_models([ngram_model, log_linear_model,cbow_model], test_data)\n",
        "\n",
        "   \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Loaded 3892 sequences from train.txt\n",
            "Vocabulary size: 13672\n",
            "Sample tokens: [774, 8, 774, 206, 88, 741, 239, 91, 137, 6]\n",
            "Sample text: Spot . Spot saw the shiny car and said ,\n",
            "Loaded training data\n",
            "Loaded 780 sequences from test.txt\n",
            "Vocabulary size: 13672\n",
            "Sample tokens: [1, 2313, 221, 166, 293, 205, 288, 8, 1, 1221]\n",
            "Sample text: \" Who are you ?\" Tim asked . \" Why\n",
            "Loaded test data\n",
            "CBOW\n",
            "the shape of contexts is torch.Size([1965035, 6])\n",
            "the shape of targets is torch.Size([1965035])\n",
            "Epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 7/61408 [00:00<16:37, 61.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 0: Loss = 17.9407\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 15/61408 [00:00<15:01, 68.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 10: Loss = 16.6517\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 22/61408 [00:00<14:59, 68.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 20: Loss = 15.5308\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 30/61408 [00:00<14:04, 72.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 30: Loss = 15.9010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 39/61408 [00:00<14:06, 72.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 40: Loss = 14.7735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 55/61408 [00:00<14:08, 72.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 50: Loss = 14.1472\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 63/61408 [00:00<13:52, 73.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 60: Loss = 12.3074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 71/61408 [00:00<14:29, 70.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 70: Loss = 13.5296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 79/61408 [00:01<14:33, 70.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 80: Loss = 13.1890\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 96/61408 [00:01<13:25, 76.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 90: Loss = 12.5856\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 104/61408 [00:01<13:51, 73.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 100: Loss = 12.8679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 113/61408 [00:01<13:26, 76.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 110: Loss = 12.0728\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 121/61408 [00:01<13:53, 73.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 120: Loss = 11.9227\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 129/61408 [00:01<14:23, 70.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 130: Loss = 10.9255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 146/61408 [00:02<13:53, 73.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 140: Loss = 9.1367\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 154/61408 [00:02<14:48, 68.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 150: Loss = 9.9206\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 163/61408 [00:02<14:14, 71.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 160: Loss = 11.1360\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 171/61408 [00:02<13:50, 73.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 170: Loss = 11.5290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 179/61408 [00:02<14:10, 72.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 180: Loss = 9.6695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 195/61408 [00:02<14:14, 71.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 190: Loss = 10.4111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 203/61408 [00:02<13:49, 73.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 200: Loss = 10.6051\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 211/61408 [00:02<13:47, 73.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 210: Loss = 9.4528\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 219/61408 [00:03<13:45, 74.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 220: Loss = 10.6605\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 228/61408 [00:03<13:17, 76.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 230: Loss = 7.0992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 244/61408 [00:03<13:51, 73.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 240: Loss = 9.4820\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 252/61408 [00:03<14:12, 71.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 250: Loss = 7.9058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 261/61408 [00:03<13:32, 75.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 260: Loss = 8.3517\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 269/61408 [00:03<13:46, 73.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 270: Loss = 9.0974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 286/61408 [00:03<13:31, 75.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 280: Loss = 8.9446\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 294/61408 [00:04<13:25, 75.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 290: Loss = 8.9368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 302/61408 [00:04<13:51, 73.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 300: Loss = 8.6143\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 311/61408 [00:04<13:18, 76.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 310: Loss = 8.7556\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 319/61408 [00:04<13:21, 76.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 320: Loss = 7.7309\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 335/61408 [00:04<13:36, 74.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 330: Loss = 7.4254\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 343/61408 [00:04<13:21, 76.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 340: Loss = 7.7109\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 351/61408 [00:04<13:24, 75.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 350: Loss = 7.1386\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 359/61408 [00:04<14:06, 72.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 360: Loss = 6.9575\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 367/61408 [00:05<14:02, 72.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 370: Loss = 8.1370\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 382/61408 [00:05<15:13, 66.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 380: Loss = 8.4983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 390/61408 [00:05<14:50, 68.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 390: Loss = 8.1069\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 398/61408 [00:05<14:29, 70.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 400: Loss = 6.8417\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 414/61408 [00:05<14:00, 72.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 410: Loss = 9.2662\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 422/61408 [00:05<15:14, 66.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 420: Loss = 7.8196\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 429/61408 [00:05<15:07, 67.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 430: Loss = 8.9732\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 446/61408 [00:06<14:12, 71.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 440: Loss = 8.5208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 454/61408 [00:06<15:08, 67.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 450: Loss = 7.4220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 461/61408 [00:06<16:08, 62.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 460: Loss = 8.0444\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 470/61408 [00:06<14:41, 69.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 470: Loss = 7.2384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 478/61408 [00:06<14:58, 67.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 480: Loss = 6.4258\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 494/61408 [00:06<14:13, 71.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 490: Loss = 6.5836\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 503/61408 [00:06<13:37, 74.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 500: Loss = 8.2591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 511/61408 [00:07<14:33, 69.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 510: Loss = 7.0052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 519/61408 [00:07<14:45, 68.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 520: Loss = 6.7398\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 536/61408 [00:07<14:00, 72.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 530: Loss = 7.7510\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 544/61408 [00:07<14:29, 69.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 540: Loss = 6.6871\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 552/61408 [00:07<14:28, 70.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 550: Loss = 7.3270\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 560/61408 [00:07<14:23, 70.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 560: Loss = 6.3533\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 569/61408 [00:07<13:49, 73.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 570: Loss = 7.4088\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 586/61408 [00:08<13:09, 77.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 580: Loss = 5.8723\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 594/61408 [00:08<13:04, 77.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 590: Loss = 6.6539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 602/61408 [00:08<13:35, 74.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 600: Loss = 6.2140\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 610/61408 [00:08<13:25, 75.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 610: Loss = 7.7942\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 618/61408 [00:08<13:15, 76.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 620: Loss = 6.9001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 634/61408 [00:08<13:55, 72.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 630: Loss = 4.9885\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 643/61408 [00:08<13:33, 74.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 640: Loss = 6.7234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 651/61408 [00:09<13:22, 75.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 650: Loss = 6.6262\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 659/61408 [00:09<14:12, 71.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 660: Loss = 5.3732\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 675/61408 [00:09<13:28, 75.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 670: Loss = 5.8738\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 683/61408 [00:09<13:27, 75.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 680: Loss = 7.5528\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 692/61408 [00:09<13:02, 77.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 690: Loss = 7.3512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 701/61408 [00:09<12:50, 78.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 700: Loss = 6.9043\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 709/61408 [00:09<13:29, 74.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 710: Loss = 6.5409\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 725/61408 [00:09<13:43, 73.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 720: Loss = 5.2853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 734/61408 [00:10<13:26, 75.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 730: Loss = 7.0013\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 742/61408 [00:10<13:29, 74.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 740: Loss = 6.8750\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 750/61408 [00:10<13:38, 74.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 750: Loss = 6.0542\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 758/61408 [00:10<13:21, 75.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 760: Loss = 7.6245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 774/61408 [00:10<13:46, 73.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 770: Loss = 6.8786\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 782/61408 [00:10<14:24, 70.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 780: Loss = 5.9996\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 790/61408 [00:10<14:21, 70.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 790: Loss = 5.7660\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 806/61408 [00:11<13:44, 73.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 800: Loss = 6.8383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 814/61408 [00:11<13:57, 72.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 810: Loss = 5.0152\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 822/61408 [00:11<13:52, 72.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 820: Loss = 7.1050\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 830/61408 [00:11<13:54, 72.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 830: Loss = 6.4770\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 838/61408 [00:11<14:23, 70.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 840: Loss = 5.8903\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 854/61408 [00:11<14:14, 70.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 850: Loss = 6.0885\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 862/61408 [00:11<15:02, 67.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 860: Loss = 6.3652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 870/61408 [00:12<14:30, 69.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 870: Loss = 5.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 879/61408 [00:12<13:49, 72.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 880: Loss = 7.6973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 895/61408 [00:12<13:21, 75.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 890: Loss = 6.4204\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 903/61408 [00:12<13:48, 73.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 900: Loss = 5.9371\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 911/61408 [00:12<13:35, 74.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 910: Loss = 5.5973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 919/61408 [00:12<13:20, 75.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 920: Loss = 6.2188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 927/61408 [00:12<13:17, 75.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 930: Loss = 6.8759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 943/61408 [00:12<13:42, 73.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 940: Loss = 5.8212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 951/61408 [00:13<14:00, 71.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 950: Loss = 5.8069\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 959/61408 [00:13<13:50, 72.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 960: Loss = 6.4239\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 975/61408 [00:13<13:39, 73.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 970: Loss = 6.8132\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 983/61408 [00:13<13:38, 73.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 980: Loss = 5.9790\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 991/61408 [00:13<14:11, 70.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 990: Loss = 4.7491\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1000/61408 [00:13<13:42, 73.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1000: Loss = 6.9914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1016/61408 [00:13<13:29, 74.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1010: Loss = 6.5831\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1024/61408 [00:14<14:04, 71.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1020: Loss = 6.0828\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1032/61408 [00:14<14:12, 70.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1030: Loss = 5.6701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1040/61408 [00:14<14:28, 69.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1040: Loss = 5.7368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1056/61408 [00:14<13:49, 72.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1050: Loss = 7.0968\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1065/61408 [00:14<13:12, 76.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1060: Loss = 4.6419\n",
            "Epoch 0, Batch 1070: Loss = 6.8965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1082/61408 [00:14<14:13, 70.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1080: Loss = 6.2603\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1090/61408 [00:15<13:55, 72.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1090: Loss = 5.5726\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1098/61408 [00:15<14:02, 71.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1100: Loss = 6.3729\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1114/61408 [00:15<13:59, 71.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1110: Loss = 5.9485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1122/61408 [00:15<14:48, 67.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1120: Loss = 4.8287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1129/61408 [00:15<15:01, 66.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1130: Loss = 6.9956\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1143/61408 [00:15<15:32, 64.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1140: Loss = 5.1928\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1150/61408 [00:15<15:30, 64.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 1150: Loss = 5.3216\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1159/61408 [00:16<13:56, 72.04it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[89], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# evaluate_models([ngram_model, log_linear_model,cbow_model], test_data)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[89], line 16\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCBOW\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m cbow_model \u001b[38;5;241m=\u001b[39m CBOWModel(tokenizer\u001b[38;5;241m.\u001b[39mget_vocab_size(), embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, context_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mcbow_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(cbow_model\u001b[38;5;241m.\u001b[39mgenerate_text(test_data[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m1\u001b[39m])))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcbow_model\u001b[38;5;241m.\u001b[39mperplexity(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[88], line 97\u001b[0m, in \u001b[0;36mCBOWModel.train\u001b[0;34m(self, token_sequences, epochs, batch_size)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# TODO: Finish the backward pass and gradient update.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Remember, you need to compute the loss, perform the backward pass, and\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# update the model parameters.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Your code here!\u001b[39;00m\n\u001b[1;32m     96\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(preds,batch_targets)\n\u001b[0;32m---> 97\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    100\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# WORD2VEC\n",
        "\n",
        "def main():\n",
        "    # Here we're training on train.txt and evaluating on test.txt.\n",
        "    # However, you might find it useful to play with tiny.txt while you're debugging.\n",
        "    # If you're running into memory issues, you can try training on a smaller set of\n",
        "    # sentences by truncating train.txt, but you should always report your final\n",
        "    # results on test.txt (and think about ways of making your code more efficient)!\n",
        "    train_data, tokenizer = load_data(\"train.txt\", tokenizer=None) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded training data\")\n",
        "    test_data, _ = load_data(\"test.txt\", tokenizer=tokenizer) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded test data\")\n",
        "\n",
        "    print('CBOW')\n",
        "    cbow_model = CBOWModel(tokenizer.get_vocab_size(), embedding_dim=100, context_size=3)\n",
        "    cbow_model.train(train_data, epochs=10)\n",
        "    print(tokenizer.decode(cbow_model.generate_text(test_data[0][:1])))\n",
        "    print(f\"Perplexity: {cbow_model.perplexity(test_data)}\")\n",
        "\n",
        "    # evaluate_models([ngram_model, log_linear_model,cbow_model], test_data)\n",
        "\n",
        "   \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "generative_ai_disabled": true,
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
