{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-Backour/6.4610-psets/blob/main/hw1_nlp_shared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CD98gW5NbOVA",
        "outputId": "1dd2362b-3580-4d58-aa65-db91f1fc94c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nHomework 1: Language Models\\n\\nThis assignment will guide you through implementing three different types of language models:\\n1. N-gram model\\n2. Log-linear model with hand-designed features\\n3. Continuous Bag of Words (CBOW) model\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Homework 1: Language Models\n",
        "\n",
        "This assignment will guide you through implementing three different types of language models:\n",
        "1. N-gram model\n",
        "2. Log-linear model with hand-designed features\n",
        "3. Continuous Bag of Words (CBOW) model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a5ooSqa9bTXF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Optional, Tuple\n",
        "import pickle\n",
        "from collections import defaultdict, Counter\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.models import BPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ywr2egPPbVQK"
      },
      "outputs": [],
      "source": [
        "class LanguageModel:\n",
        "    \"\"\"\n",
        "    Abstract base class for language models.\n",
        "\n",
        "    This class defines the interface that all language models should implement.\n",
        "    Language models take a sequence of tokens (represented as integers) and\n",
        "    predict probability distributions over the next token in the vocabulary.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.eos = 'eos'\n",
        "\n",
        "    def train(self, token_sequences: List[np.ndarray]) -> None:\n",
        "        \"\"\"\n",
        "        Train the language model on a collection of token sequences.\n",
        "\n",
        "        Args:\n",
        "            token_sequences (List[np.ndarray]): List of token sequences, where each\n",
        "                                               sequence is a numpy array of integers\n",
        "                                               representing token IDs.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement the train method\")\n",
        "\n",
        "    def get_next_token_probs(self, context: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get probability distribution over next tokens given a context.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Array of token IDs representing the context.\n",
        "                                 Shape: (context_length,)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Probability distribution over vocabulary.\n",
        "                       Shape: (vocab_size,)\n",
        "                       Should sum to 1.0.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement the get_next_token_probs method\")\n",
        "\n",
        "    def perplexity(self, token_sequences: List[np.ndarray]) -> float:\n",
        "        \"\"\"\n",
        "        Calculate perplexity of the model on a set of token sequences.\n",
        "\n",
        "        Perplexity is 2^(-average_log_likelihood), where average log likelihood\n",
        "        is calculated over all tokens in all sequences.\n",
        "\n",
        "        Args:\n",
        "            token_sequences (List[np.ndarray]): List of token sequences to evaluate\n",
        "\n",
        "        Returns:\n",
        "            float: Perplexity score (lower is better)\n",
        "        \"\"\"\n",
        "        total_log_likelihood = 0.0\n",
        "        total_tokens = 0\n",
        "\n",
        "        for sequence in token_sequences:\n",
        "            if len(sequence) < 2:\n",
        "                continue  # Need at least 2 tokens (context + target)\n",
        "\n",
        "            for i in range(1, len(sequence)):\n",
        "                context = sequence[:i]\n",
        "                target_token = sequence[i]\n",
        "\n",
        "                probs = self.get_next_token_probs(context)\n",
        "                # Add small epsilon to avoid log(0)\n",
        "                prob = max(float(probs[target_token]), 1e-10)\n",
        "                total_log_likelihood += np.log2(prob)\n",
        "                total_tokens += 1\n",
        "\n",
        "        if total_tokens == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        average_log_likelihood = total_log_likelihood / total_tokens\n",
        "        return 2 ** (-average_log_likelihood)\n",
        "\n",
        "    def generate_text(self, context: np.ndarray, max_length: int = 100,\n",
        "                     temperature: float = 1.0) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate text by sampling from the model.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Initial context tokens\n",
        "            max_length (int): Maximum number of tokens to generate\n",
        "            temperature (float): Sampling temperature (higher = more random)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Generated sequence including the initial context\n",
        "        \"\"\"\n",
        "        generated = list(context)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            current_context = np.array(generated)\n",
        "            probs = self.get_next_token_probs(current_context)\n",
        "\n",
        "            # Apply temperature\n",
        "            if temperature != 1.0:\n",
        "                probs = np.power(probs, 1.0 / temperature)\n",
        "                probs = probs / np.sum(probs)\n",
        "\n",
        "            # Sample next token\n",
        "            next_token = np.random.choice(len(probs), p=probs)\n",
        "            generated.append(next_token)\n",
        "\n",
        "            # Optional: add stopping criteria here (e.g., end-of-sequence token)\n",
        "            if next_token == self.eos:\n",
        "              break\n",
        "\n",
        "        return np.array(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "qqLcUlo4bY4-"
      },
      "outputs": [],
      "source": [
        "class NGramModel(LanguageModel):\n",
        "    \"\"\"\n",
        "    N-gram language model using maximum likelihood estimation with smoothing.\n",
        "\n",
        "    This model predicts the next token based on the previous n-1 tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, n: int = 3, smoothing=None):\n",
        "        \"\"\"\n",
        "        Initialize the N-gram model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            n (int): Order of the n-gram (default: 3 for trigram)\n",
        "\n",
        "            Bonus! Without smoothing your perplexity will be pretty bad. If you\n",
        "            implement some kind of smoothing and get the perplexity below 300\n",
        "            you'll get extra credit.\n",
        "            smoothing (str): Smoothing method ('laplace' or 'interpolation')\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Hint: Consider using nested dictionaries or defaultdict(Counter) to store counts.\n",
        "        # Hint: Consider how you will handle different context lengths.\n",
        "        # At the start of a sentence, you might have 0, 1, or 2 words of context\n",
        "        # instead of the full n-1 words.\n",
        "        self.n = n\n",
        "        self.vocab_size = vocab_size\n",
        "        self.counts = {}\n",
        "    def train(self, token_sequences: List[np.ndarray]) -> None:\n",
        "        \"\"\"\n",
        "        Train the n-gram model by counting n-grams in the training data.\n",
        "\n",
        "        Args:\n",
        "            token_sequences (List[np.ndarray]): Training sequences\n",
        "        \"\"\"\n",
        "        for doc in token_sequences:\n",
        "            for i in range(len(doc) - self.n + 1):\n",
        "                for j in range(self.n):\n",
        "                    window = tuple(doc[i:i+j+1])\n",
        "                    self.counts.setdefault(window,0)\n",
        "                    self.counts[window] +=1\n",
        "\n",
        "\n",
        "    def get_next_token_probs(self, context: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get probability distribution over next tokens for given context.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Context tokens\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Probability distribution over vocabulary\n",
        "        \"\"\"\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Hint: Try contexts from longest to shortest, i.e., try the full context,\n",
        "        # and if it is not in the training data, try shorter context\n",
        "        # Hint: What probability distribution should we output if no valid context is found?\n",
        "        for i in range(self.n - 1,0 ,-1):\n",
        "            window = tuple(context[-i:])\n",
        "            if window in self.counts:\n",
        "                prb =  np.array([self.counts.get(window + (new_word,),0)/self.counts[window] for new_word in range(self.vocab_size)])\n",
        "                return prb/sum(prb)\n",
        "        prb =  np.array([self.counts.get(new_word,0) for new_word in range(self.vocab_size)])\n",
        "        if sum(prb):\n",
        "            return prb/sum(prb)\n",
        "        return np.array([1/self.vocab_size for _ in range(self.vocab_size)])\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Celb4A0sbkAN"
      },
      "outputs": [],
      "source": [
        "class LogLinearModel(LanguageModel):\n",
        "    \"\"\"\n",
        "    Log-linear language model with hand-designed features.\n",
        "\n",
        "    This model uses a linear combination of features to predict next token probabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, context_size: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the log-linear model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            context_size (int): Number of context tokens to consider\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: YOUR CODE HERE\n",
        "\n",
        "        self.context_size = context_size\n",
        "        self.vocab_size = vocab_size\n",
        "        # Hint: You may find the class nn.Linear useful\n",
        "        # If this is too slow or using too much memory, check out the nn.EmbeddingBag class\n",
        "        # and see if that's applicable to your use case\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(context_size * vocab_size,vocab_size)\n",
        "            )\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01)\n",
        "\n",
        "    def extract_features(self, context: np.ndarray) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Extract features from the context.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Context tokens\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Feature vector produced from context tokens\n",
        "        \"\"\"\n",
        "        features = torch.zeros(self.vocab_size,self.context_size,dtype=torch.float32)\n",
        "        relevant_context = context[-self.context_size:]\n",
        "        for i,word in enumerate(relevant_context):\n",
        "            features[int(word),i] = 1.0\n",
        "        return features.view(-1)\n",
        "\n",
        "    def train(self, token_sequences: List[np.ndarray], epochs: int = 2, batch_size: int = 32) -> None:\n",
        "        \"\"\"\n",
        "        Train the log-linear model using gradient descent.\n",
        "        \"\"\"\n",
        "        # Create training examples (context, target) pairs\n",
        "        contexts_list = []\n",
        "        targets_list = []\n",
        "        for seq in token_sequences:\n",
        "            contexts_list.extend(seq[i:i+self.context_size] for i in range(len(seq) - self.context_size))\n",
        "            targets_list.extend(seq[self.context_size:])\n",
        "        assert len(contexts_list) == len(targets_list)\n",
        "        all_features = contexts_list\n",
        "        # TODO: YOUR CODE HERE\n",
        "\n",
        "\n",
        "        print(f\"Training on {len(all_features)} examples for {epochs} epochs...\")\n",
        "\n",
        "        # Training loop\n",
        "        # TODO: Put your layers in training mode\n",
        "        self.model.train()\n",
        "        losses = []  # Potentially useful for debugging (loss should go down!)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(device)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch}\")\n",
        "            total_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            # Mini-batch training\n",
        "            # Note: tqdm is used to display progress bars for loops, helping visualize training progress.\n",
        "            for i in tqdm(range(0, len(all_features), batch_size)):\n",
        "                batch_contexts= contexts_list[i:i+batch_size]\n",
        "                batch_targets = targets_list[i:i+batch_size]\n",
        "\n",
        "                # TODO: Get features for the batch\n",
        "                batch_contexts = torch.stack([self.extract_features(c) for c in batch_contexts])\n",
        "                batch_contexts = batch_contexts.to(device)\n",
        "                batch_targets = torch.tensor(batch_targets,device=device,dtype=torch.int64)\n",
        "                # TODO: Zero the gradients of the optimizer\n",
        "                \n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "                # TODO: Perform a forward pass to compute predictions for the model.\n",
        "                logits = self.model(batch_contexts)\n",
        "\n",
        "\n",
        "                # TODO: Perform the backward pass and gradient update. Remember,\n",
        "                # you need to compute the loss, perform the backward pass, and\n",
        "                # update the model parameters.\n",
        "                # Your code here!\n",
        "                loss = self.criterion(logits,batch_targets)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "                if i % (batch_size * 10) == 0:  # Print every 10 batches\n",
        "                    print(f\"Epoch {epoch}, Batch {i // batch_size}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "            if epoch % 2 == 0:\n",
        "                avg_loss = total_loss / num_batches\n",
        "                print(f\"Epoch {epoch}: Average Loss = {avg_loss:.4f}\")\n",
        "\n",
        "            losses.append(total_loss)\n",
        "        torch.save(self.model.state_dict(),'log_linear.pth')\n",
        "\n",
        "    def get_next_token_probs(self, context: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get probability distribution using softmax over linear scores.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Context tokens\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Probability distribution over vocabulary\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            if len(context) == 0:\n",
        "                return torch.full((self.vocab_size,),1.0/self.vocab_size)\n",
        "            features = self.extract_features(context).unsqueeze(0)\n",
        "            features = features.to(next(self.model.parameters()).device)\n",
        "            logits = self.model(features)\n",
        "            prb = torch.softmax(logits, dim=-1).cpu().numpy().flatten()\n",
        "        return prb\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Hint: What probability distribution should we output if no valid context is found?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT1D2snWcHSy"
      },
      "outputs": [],
      "source": [
        "class CBOWModeler(torch.nn.Module):\n",
        "    def __init__(self,vocab_size,embedding_dim,context_size,hidden_dim = 128):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.context_size = context_size\n",
        "        self.w = nn.Embedding(vocab_size,embedding_dim)\n",
        "        self.linear_layer_1 = torch.nn.Linear(embedding_dim,hidden_dim)\n",
        "        self.linear_layer_2 = torch.nn.Linear(hidden_dim,vocab_size)\n",
        "\n",
        "    def forward(self,x):\n",
        "        cbow_emb = self.w(x)\n",
        "        avg = torch.mean(cbow_emb,dim=1)\n",
        "        output = self.linear_layer_1(avg)\n",
        "        output = torch.nn.functional.relu(output)\n",
        "        output = self.linear_layer_2(output)\n",
        "        return output\n",
        "class CBOWModel(LanguageModel):\n",
        "    \"\"\"\n",
        "    Continuous Bag of Words (CBOW) model.\n",
        "\n",
        "    This model learns dense vector representations of words and predicts\n",
        "    the next word from the context words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int = 100, context_size: int = 2, learning_rate: float = 0.01):\n",
        "        \"\"\"\n",
        "        Initialize the CBOW model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            embedding_dim (int): Dimension of word embeddings\n",
        "            context_size (int): Number of context words on each side\n",
        "            learning_rate (float): Learning rate for training\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # You may find the classes nn.Embedding and nn.EmbeddingBag useful\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.context_size = context_size\n",
        "        self.model = CBOWModeler(vocab_size,embedding_dim,context_size)\n",
        "        # We use an Adam optimizer. This is a fancy version of SGD which uses momentum and adaptive updates.\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # What loss function should we use for Word2Vec?\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "        \n",
        "    def train(self, token_sequences: List[np.ndarray], epochs: int = 10, batch_size: int = 32) -> None:\n",
        "        \"\"\"\n",
        "        Train the CBOW model.\n",
        "\n",
        "        Args:\n",
        "            token_sequences (List[np.ndarray]): Training sequences\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "        \"\"\"\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Create training examples (context, target) pairs\n",
        "        # Hint, extract left context only for next token prediction\n",
        "        # Hint, pad shorter contexts with 0, this ensures all have the same length for batching\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        contexts_list = []\n",
        "        targets_list = []\n",
        "        for seq in token_sequences:\n",
        "            contexts_list.extend(np.concatenate((seq[i - self.context_size : i],seq[i+1:i + self.context_size + 1])) for i in range(self.context_size, len(seq)-self.context_size - 1))\n",
        "            targets_list.extend([seq[i] for i in range(self.context_size, len(seq)-self.context_size - 1)])\n",
        "        assert len(contexts_list) == len(targets_list)\n",
        "        all_contexts = torch.tensor(contexts_list).to(device=device)\n",
        "        all_targets = torch.tensor(targets_list).to(device=device)\n",
        "        print(f\"the shape of contexts is {all_contexts.shape}\")\n",
        "        print(f\"the shape of targets is {all_targets.shape}\")\n",
        "\n",
        "        # TODO: Put your layers in training mode\n",
        "        self.model.train()\n",
        "        self.model.to(device=device)\n",
        "        losses = []  # Potentially useful for debugging (loss should go down!)\n",
        "        # Note: tqdm is used to display progress bars for loops, helping visualize training progress.\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch}\")\n",
        "            total_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            # Shuffle data\n",
        "            indices = torch.randperm(len(all_contexts))\n",
        "            all_contexts = all_contexts[indices]\n",
        "            all_targets = all_targets[indices]\n",
        "\n",
        "            for i in tqdm(range(0, len(all_contexts), batch_size)):\n",
        "                # As an alternative to this implementation, you can experiment with\n",
        "                # DataLoader (https://docs.pytorch.org/docs/stable/data.html) for automatic shuffling, parallel loading\n",
        "                batch_contexts = all_contexts[i:i+batch_size]\n",
        "                batch_targets = all_targets[i:i+batch_size]\n",
        "\n",
        "                # TODO: Zero the gradients of the optimizer\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # TODO: Perform a forward pass to compute predictions for the model.\n",
        "                # Your code here!\n",
        "                preds = self.model(batch_contexts)\n",
        "                # TODO: Finish the backward pass and gradient update.\n",
        "                # Remember, you need to compute the loss, perform the backward pass, and\n",
        "                # update the model parameters.\n",
        "                # Your code here!\n",
        "                loss = self.criterion(preds,batch_targets)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "                if i % (batch_size * 10) == 0:  # Print every 10 batches\n",
        "                    print(f\"Epoch {epoch}, Batch {i // batch_size}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "            if epoch % 2 == 0:\n",
        "                avg_loss = total_loss / num_batches\n",
        "                print(f\"Epoch {epoch}: Average Loss = {avg_loss:.4f}\")\n",
        "\n",
        "            losses.append(total_loss)\n",
        "        torch.save(self.model.state_dict(),'cbow_model.pth')\n",
        "\n",
        "    def get_next_token_probs(self, context: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get next-token probability distributions.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Context tokens\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Probability distribution over vocabulary\n",
        "        \"\"\"\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Hints:\n",
        "        # For next-token prediction, we use the last context_size tokens\n",
        "        # No valid context, return uniform distribution\n",
        "        # Pad context to expected size\n",
        "        # Don't forget to add batch dimension, torch expects (batch_size, context_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if len(context) == 0:\n",
        "                return torch.full((self.vocab_size,),1.0/self.vocab_size).cpu().numpy()\n",
        "            padded_context = np.pad(context,(self.context_size,0),mode = 'constant')\n",
        "            relevant = torch.tensor(padded_context[-self.context_size:]).to(device=next(self.model.parameters()).device)\n",
        "            logits = self.model(relevant.unsqueeze(0))\n",
        "            return torch.softmax(logits,dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "\n",
        "    def get_word_embedding(self, token_id: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get the learned embedding for a specific token.\n",
        "\n",
        "        Args:\n",
        "            token_id (int): Token ID\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Word embedding vector\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.w(token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_3fWkVnxcXu4"
      },
      "outputs": [],
      "source": [
        "def load_data(filepath: str, tokenizer: Optional[Tokenizer], max_seq_length: int = 512) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load and preprocess text data using GPT-2 tokenizer.\n",
        "\n",
        "    This function is provided complete - students don't need to modify it.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the text file\n",
        "        tokenizer (Optional[Tokenizer]): Tokenizer to use. If None, a new tokenizer will be created.\n",
        "        max_seq_length (int): Maximum sequence length for splitting text\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[np.ndarray], GPT2Tokenizer]: List of token sequences and the tokenizer\n",
        "    \"\"\"\n",
        "    # Byte Pair Encoding (BPE)\n",
        "\n",
        "    if tokenizer is None:\n",
        "        tokenizer = Tokenizer(BPE())\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        #trainer = BpeTrainer(special_tokens=[\"[PAD]\"])\n",
        "        tokenizer.train([filepath])\n",
        "\n",
        "    # Read the text file\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Tokenize the entire text\n",
        "    tokens = tokenizer.encode(text).ids\n",
        "\n",
        "    # Split into sequences of max_seq_length\n",
        "    sequences = []\n",
        "    for i in range(0, len(tokens), max_seq_length):\n",
        "        sequence = tokens[i:i+max_seq_length]\n",
        "        if len(sequence) > 1:  # Need at least 2 tokens for language modeling\n",
        "            sequences.append(np.array(sequence))\n",
        "\n",
        "    print(f\"Loaded {len(sequences)} sequences from {filepath}\")\n",
        "    print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
        "    print(f\"Sample tokens: {tokens[:10]}\")\n",
        "    print(f\"Sample text: {tokenizer.decode(tokens[:10])}\")\n",
        "\n",
        "    return sequences, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0ItN7U7ocbkA"
      },
      "outputs": [],
      "source": [
        "def evaluate_models(models: List[LanguageModel], test_data: List[np.ndarray]) -> None:\n",
        "    \"\"\"\n",
        "    Evaluate and compare multiple language models.\n",
        "\n",
        "    Args:\n",
        "        models (List[LanguageModel]): List of trained models\n",
        "        test_data (List[np.ndarray]): Test sequences\n",
        "    \"\"\"\n",
        "    print(\"Model Evaluation Results:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        model_name = model.__class__.__name__\n",
        "        try:\n",
        "            ppl = model.perplexity(test_data)\n",
        "            print(f\"{model_name}: Perplexity = {ppl:.2f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"{model_name}: Error calculating perplexity - {e}\")\n",
        "\n",
        "    print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xMtmXsBVvcWX"
      },
      "outputs": [],
      "source": [
        "def analyze(model1: LanguageModel, model2: LanguageModel, test_data: List[np.ndarray],\n",
        "           tokenizer=None, context_length: int = 2) -> dict:\n",
        "    \"\"\"\n",
        "    Compare two models and find contexts where each performs better.\n",
        "\n",
        "    Args:\n",
        "        model1 (LanguageModel): First model to compare\n",
        "        model2 (LanguageModel): Second model to compare\n",
        "        test_data (List[np.ndarray]): Test sequences\n",
        "        tokenizer: Tokenizer for decoding (optional, for display purposes)\n",
        "        context_length (int): Context length to consider\n",
        "\n",
        "    Returns:\n",
        "        dict: Analysis results including overall perplexities and context comparisons\n",
        "    \"\"\"\n",
        "    print(\"Detailed Model Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Overall perplexity comparison\n",
        "    try:\n",
        "        ppl1 = model1.perplexity(test_data)\n",
        "        ppl2 = model2.perplexity(test_data)\n",
        "\n",
        "        model1_name = model1.__class__.__name__\n",
        "        model2_name = model2.__class__.__name__\n",
        "\n",
        "        print(f\"{model1_name} overall perplexity: {ppl1:.3f}\")\n",
        "        print(f\"{model2_name} overall perplexity: {ppl2:.3f}\")\n",
        "        print(f\"Better overall model: {model1_name if ppl1 < ppl2 else model2_name}\")\n",
        "        print()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating overall perplexity: {e}\")\n",
        "        return {}\n",
        "\n",
        "    # Context-level analysis\n",
        "    context_comparisons = []\n",
        "    model1_better_contexts = []\n",
        "    model2_better_contexts = []\n",
        "\n",
        "    print(\"Analyzing context-level performance...\")\n",
        "\n",
        "    for seq_idx, sequence in enumerate(test_data):\n",
        "        for i in range(context_length, len(sequence)):\n",
        "            context = sequence[i - context_length:i]\n",
        "            target_token = sequence[i]\n",
        "\n",
        "            try:\n",
        "                # Get predictions from both models\n",
        "                probs1 = model1.get_next_token_probs(context)\n",
        "                probs2 = model2.get_next_token_probs(context)\n",
        "\n",
        "                # Calculate log probabilities for the actual target\n",
        "                prob1 = max(probs1[target_token], 1e-10)\n",
        "                prob2 = max(probs2[target_token], 1e-10)\n",
        "\n",
        "                log_prob1 = np.log2(prob1)\n",
        "                log_prob2 = np.log2(prob2)\n",
        "\n",
        "                # Store comparison data\n",
        "                context_info = {\n",
        "                    'context': context.copy(),\n",
        "                    'target': target_token,\n",
        "                    'log_prob1': log_prob1,\n",
        "                    'log_prob2': log_prob2,\n",
        "                    'seq_idx': seq_idx,\n",
        "                    'pos': i\n",
        "                }\n",
        "                context_comparisons.append(context_info)\n",
        "\n",
        "                # Categorize based on which model is better\n",
        "                if log_prob1 > log_prob2:  # Higher log prob = better\n",
        "                    model1_better_contexts.append(context_info)\n",
        "                else:\n",
        "                    model2_better_contexts.append(context_info)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error analyzing context at seq {seq_idx}, pos {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Calculate statistics\n",
        "    total_contexts = len(context_comparisons)\n",
        "    model1_wins = len(model1_better_contexts)\n",
        "    model2_wins = len(model2_better_contexts)\n",
        "\n",
        "    print(f\"Total contexts analyzed: {total_contexts}\")\n",
        "    print(f\"{model1_name} better contexts: {model1_wins} ({100*model1_wins/total_contexts:.1f}%)\")\n",
        "    print(f\"{model2_name} better contexts: {model2_wins} ({100*model2_wins/total_contexts:.1f}%)\")\n",
        "    print()\n",
        "\n",
        "    # Find patterns in contexts where each model excels\n",
        "    def analyze_context_patterns(better_contexts, model_name, top_k=10):\n",
        "        print(f\"Top {top_k} unique contexts where {model_name} excels:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Group contexts by (context, target) pairs\n",
        "        context_groups = {}\n",
        "        for ctx_info in better_contexts:\n",
        "            context_tuple = tuple(ctx_info['context'])\n",
        "            target = ctx_info['target']\n",
        "            key = (context_tuple, target)\n",
        "\n",
        "            if key not in context_groups:\n",
        "                context_groups[key] = {\n",
        "                    'contexts': [],\n",
        "                    'best_diff': 0,\n",
        "                    'context': ctx_info['context'],\n",
        "                    'target': target\n",
        "                }\n",
        "\n",
        "            context_groups[key]['contexts'].append(ctx_info)\n",
        "\n",
        "            # Track the best performance difference for this context\n",
        "            diff = (ctx_info['log_prob1'] - ctx_info['log_prob2']\n",
        "                   if model_name == model1_name\n",
        "                   else ctx_info['log_prob2'] - ctx_info['log_prob1'])\n",
        "\n",
        "            if diff > context_groups[key]['best_diff']:\n",
        "                context_groups[key]['best_diff'] = diff\n",
        "\n",
        "        # Sort by best performance difference\n",
        "        sorted_groups = sorted(context_groups.values(),\n",
        "                             key=lambda x: x['best_diff'],\n",
        "                             reverse=True)\n",
        "\n",
        "        for i, group in enumerate(sorted_groups[:top_k]):\n",
        "            context = group['context']\n",
        "            target = group['target']\n",
        "            count = len(group['contexts'])\n",
        "            best_diff = group['best_diff']\n",
        "\n",
        "            # Format context display\n",
        "            if tokenizer is not None:\n",
        "                try:\n",
        "                    context_text = tokenizer.decode(context[-min(5, len(context)):])\n",
        "                    target_text = tokenizer.decode([target])\n",
        "                    print(f\"{i+1:2d}. Context: '{context_text}' → Target: '{target_text}' (×{count})\")\n",
        "                except:\n",
        "                    print(f\"{i+1:2d}. Context: {context[-min(5, len(context)):]} → Target: {target} (×{count})\")\n",
        "            else:\n",
        "                print(f\"{i+1:2d}. Context: {context[-min(5, len(context)):]} → Target: {target} (×{count})\")\n",
        "\n",
        "            # Show best probability difference for this context type\n",
        "            print(f\"     Best log-prob difference: {best_diff:.3f}\")\n",
        "\n",
        "            # If there are multiple instances, show average difference\n",
        "            if count > 1:\n",
        "                avg_diff = sum((ctx['log_prob1'] - ctx['log_prob2']\n",
        "                              if model_name == model1_name\n",
        "                              else ctx['log_prob2'] - ctx['log_prob1'])\n",
        "                             for ctx in group['contexts']) / count\n",
        "                print(f\"     Average log-prob difference: {avg_diff:.3f}\")\n",
        "            print()\n",
        "\n",
        "    # Analyze patterns for both models\n",
        "    if model1_better_contexts:\n",
        "        analyze_context_patterns(model1_better_contexts, model1_name)\n",
        "\n",
        "    if model2_better_contexts:\n",
        "        analyze_context_patterns(model2_better_contexts, model2_name)\n",
        "\n",
        "    # Analyze context length effects\n",
        "    print(\"Performance by context length:\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    context_length_stats = {}\n",
        "    for ctx_info in context_comparisons:\n",
        "        ctx_len = len(ctx_info['context'])\n",
        "        if ctx_len not in context_length_stats:\n",
        "            context_length_stats[ctx_len] = {'model1_wins': 0, 'model2_wins': 0, 'total': 0}\n",
        "\n",
        "        context_length_stats[ctx_len]['total'] += 1\n",
        "        if ctx_info['log_prob1'] > ctx_info['log_prob2']:\n",
        "            context_length_stats[ctx_len]['model1_wins'] += 1\n",
        "        else:\n",
        "            context_length_stats[ctx_len]['model2_wins'] += 1\n",
        "\n",
        "    for ctx_len in sorted(context_length_stats.keys()):\n",
        "        stats = context_length_stats[ctx_len]\n",
        "        model1_pct = 100 * stats['model1_wins'] / stats['total']\n",
        "        model2_pct = 100 * stats['model2_wins'] / stats['total']\n",
        "        print(f\"Length {ctx_len:2d}: {model1_name} {model1_pct:5.1f}% | {model2_name} {model2_pct:5.1f}% ({stats['total']} examples)\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Return structured results\n",
        "    return {\n",
        "        'overall_perplexity': {model1_name: ppl1, model2_name: ppl2},\n",
        "        'context_level': {\n",
        "            'total_contexts': total_contexts,\n",
        "            f'{model1_name}_wins': model1_wins,\n",
        "            f'{model2_name}_wins': model2_wins,\n",
        "            f'{model1_name}_better_contexts': model1_better_contexts[:10],  # Top 10\n",
        "            f'{model2_name}_better_contexts': model2_better_contexts[:10],  # Top 10\n",
        "        },\n",
        "        'context_length_stats': context_length_stats\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "MNOveioKceQH",
        "outputId": "329b849f-11e4-4e02-f97e-80eb9c3edad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Loaded 3892 sequences from train.txt\n",
            "Vocabulary size: 13672\n",
            "Sample tokens: [774, 8, 774, 206, 88, 741, 239, 91, 137, 6]\n",
            "Sample text: Spot . Spot saw the shiny car and said ,\n",
            "Loaded training data\n",
            "Loaded 780 sequences from test.txt\n",
            "Vocabulary size: 13672\n",
            "Sample tokens: [1, 2313, 221, 166, 293, 205, 288, 8, 1, 1221]\n",
            "Sample text: \" Who are you ?\" Tim asked . \" Why\n",
            "Loaded test data\n",
            "N-gram\n",
            "\" Okay , mom .\" Their parents were very important that Mrs . Lung smiled again because he couldn ' t be selfish . She also put some magic to hide behind a tree , she found some yummy food on it . To the kitten were best friends . He only hears his tummy . She loved to jump so much fun . He was three years old and she couldn ' t believe his luck never ran out of the hill he became a good idea and they were never miserable again . The new flower in the pond\n",
            "Perplexity: 692.5371509498468\n"
          ]
        }
      ],
      "source": [
        "# Feel free to comment out portions of the code and run it multiple times, or to\n",
        "# take it out of the main() function. If you're struggling to lower your\n",
        "# perplexity, you can play around with the model hyperparameters like the\n",
        "# learning rate, batch size, and number of epochs.\n",
        "\n",
        "def main():\n",
        "    # Here we're training on train.txt and evaluating on test.txt.\n",
        "    # However, you might find it useful to play with tiny.txt while you're debugging.\n",
        "    # If you're running into memory issues, you can try training on a smaller set of\n",
        "    # sentences by truncating train.txt, but you should always report your final\n",
        "    # results on test.txt (and think about ways of making your code more efficient)!\n",
        "\n",
        "    train_data, tokenizer = load_data(\"train.txt\", tokenizer=None) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded training data\")\n",
        "    test_data, _ = load_data(\"test.txt\", tokenizer=tokenizer) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded test data\")\n",
        "\n",
        "    print('N-gram')\n",
        "    ngram_model = NGramModel(tokenizer.get_vocab_size(), n=3)\n",
        "    ngram_model.train(train_data)\n",
        "    print(tokenizer.decode(ngram_model.generate_text(test_data[0][:1])))\n",
        "    print(f\"Perplexity: {ngram_model.perplexity(test_data)}\")\n",
        "\n",
        "    # print('Log-linear')\n",
        "    # log_linear_model = LogLinearModel(tokenizer.get_vocab_size())\n",
        "    # log_linear_model.train(train_data)\n",
        "    # print(tokenizer.decode(log_linear_model.generate_text(test_data[0][:1])))\n",
        "    # print(f\"Perplexity: {log_linear_model.perplexity(test_data)}\")\n",
        "\n",
        "    # print('CBOW')\n",
        "    # cbow_model = CBOWModel(tokenizer.get_vocab_size(), embedding_dim=100, context_size=3)\n",
        "    # cbow_model.train(train_data, epochs=10)\n",
        "    # print(tokenizer.decode(cbow_model.generate_text(test_data[0][:1])))\n",
        "    # print(f\"Perplexity: {cbow_model.perplexity(test_data)}\")\n",
        "\n",
        "    # evaluate_models([ngram_model, log_linear_model,cbow_model], test_data)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Loaded 3892 sequences from train.txt\n",
            "Vocabulary size: 13672\n",
            "Sample tokens: [774, 8, 774, 206, 88, 741, 239, 91, 137, 6]\n",
            "Sample text: Spot . Spot saw the shiny car and said ,\n",
            "Loaded training data\n",
            "Loaded 780 sequences from test.txt\n",
            "Vocabulary size: 13672\n",
            "Sample tokens: [1, 2313, 221, 166, 293, 205, 288, 8, 1, 1221]\n",
            "Sample text: \" Who are you ?\" Tim asked . \" Why\n",
            "Loaded test data\n",
            "Log-linear\n",
            "Training on 1980603 examples for 2 epochs...\n",
            "Epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/61894 [00:07<123:55:01,  7.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 0: Loss = 9.5231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 7/61894 [00:45<112:39:40,  6.55s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[45], line 30\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# print('CBOW')\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# cbow_model = CBOWModel(tokenizer.get_vocab_size(), embedding_dim=100, context_size=3)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# cbow_model.train(train_data, epochs=10)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# evaluate_models([ngram_model, log_linear_model,cbow_model], test_data)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[45], line 16\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLog-linear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m log_linear_model \u001b[38;5;241m=\u001b[39m LogLinearModel(tokenizer\u001b[38;5;241m.\u001b[39mget_vocab_size())\n\u001b[0;32m---> 16\u001b[0m \u001b[43mlog_linear_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(log_linear_model\u001b[38;5;241m.\u001b[39mgenerate_text(test_data[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m1\u001b[39m])))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_linear_model\u001b[38;5;241m.\u001b[39mperplexity(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[44], line 99\u001b[0m, in \u001b[0;36mLogLinearModel.train\u001b[0;34m(self, token_sequences, epochs, batch_size)\u001b[0m\n\u001b[1;32m     97\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(logits,batch_targets)\n\u001b[1;32m     98\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    101\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/adam.py:478\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 478\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# LOG LINEAR MODEL\n",
        "\n",
        "def main():\n",
        "    # Here we're training on train.txt and evaluating on test.txt.\n",
        "    # However, you might find it useful to play with tiny.txt while you're debugging.\n",
        "    # If you're running into memory issues, you can try training on a smaller set of\n",
        "    # sentences by truncating train.txt, but you should always report your final\n",
        "    # results on test.txt (and think about ways of making your code more efficient)!\n",
        "    train_data, tokenizer = load_data(\"train.txt\", tokenizer=None) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded training data\")\n",
        "    test_data, _ = load_data(\"test.txt\", tokenizer=tokenizer) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded test data\")\n",
        "\n",
        "    print('Log-linear')\n",
        "    log_linear_model = LogLinearModel(tokenizer.get_vocab_size())\n",
        "    log_linear_model.train(train_data)\n",
        "    print(tokenizer.decode(log_linear_model.generate_text(test_data[0][:1])))\n",
        "    print(f\"Perplexity: {log_linear_model.perplexity(test_data)}\")\n",
        "\n",
        "    # evaluate_models([ngram_model, log_linear_model,cbow_model], test_data)\n",
        "\n",
        "   \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Loaded 3892 sequences from train.txt\n",
            "Vocabulary size: 13672\n",
            "Sample tokens: [774, 8, 774, 206, 88, 741, 239, 91, 137, 6]\n",
            "Sample text: Spot . Spot saw the shiny car and said ,\n",
            "Loaded training data\n",
            "Loaded 780 sequences from test.txt\n",
            "Vocabulary size: 13672\n",
            "Sample tokens: [1, 2313, 221, 166, 293, 205, 288, 8, 1, 1221]\n",
            "Sample text: \" Who are you ?\" Tim asked . \" Why\n",
            "Loaded test data\n",
            "CBOW\n",
            "the shape of contexts is torch.Size([1965035, 6])\n",
            "the shape of targets is torch.Size([1965035])\n",
            "Epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 5/61408 [00:00<21:26, 47.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 0: Loss = 9.5129\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 13/61408 [00:00<15:45, 64.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 10: Loss = 7.4204\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 21/61408 [00:00<15:11, 67.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 20: Loss = 8.2785\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 29/61408 [00:00<14:41, 69.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 30: Loss = 7.5693\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 45/61408 [00:00<14:12, 71.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 40: Loss = 6.5979\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 53/61408 [00:00<14:07, 72.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 50: Loss = 7.9027\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 61/61408 [00:00<14:25, 70.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 60: Loss = 6.8834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 69/61408 [00:01<15:38, 65.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 70: Loss = 6.7248\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 85/61408 [00:01<14:46, 69.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 80: Loss = 7.1362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 93/61408 [00:01<14:18, 71.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 90: Loss = 5.4254\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 101/61408 [00:01<14:15, 71.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 100: Loss = 6.1006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 110/61408 [00:01<13:34, 75.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 110: Loss = 7.4680\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 126/61408 [00:01<13:22, 76.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 120: Loss = 5.5967\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 133/61408 [00:01<14:15, 71.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 130: Loss = 5.6883\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[104], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# evaluate_models([ngram_model, log_linear_model,cbow_model], test_data)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[104], line 16\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCBOW\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m cbow_model \u001b[38;5;241m=\u001b[39m CBOWModel(tokenizer\u001b[38;5;241m.\u001b[39mget_vocab_size(), embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, context_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mcbow_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(cbow_model\u001b[38;5;241m.\u001b[39mgenerate_text(test_data[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m1\u001b[39m])))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcbow_model\u001b[38;5;241m.\u001b[39mperplexity(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[103], line 100\u001b[0m, in \u001b[0;36mCBOWModel.train\u001b[0;34m(self, token_sequences, epochs, batch_size)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# TODO: Perform a forward pass to compute predictions for the model.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Your code here!\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_contexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# TODO: Finish the backward pass and gradient update.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Remember, you need to compute the loss, perform the backward pass, and\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# update the model parameters.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Your code here!\u001b[39;00m\n\u001b[1;32m    105\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(preds,batch_targets)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[103], line 16\u001b[0m, in \u001b[0;36mCBOWModeler.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_layer_1(avg)\n\u001b[1;32m     15\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(output)\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_layer_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# WORD2VEC\n",
        "\n",
        "def main():\n",
        "    # Here we're training on train.txt and evaluating on test.txt.\n",
        "    # However, you might find it useful to play with tiny.txt while you're debugging.\n",
        "    # If you're running into memory issues, you can try training on a smaller set of\n",
        "    # sentences by truncating train.txt, but you should always report your final\n",
        "    # results on test.txt (and think about ways of making your code more efficient)!\n",
        "    train_data, tokenizer = load_data(\"train.txt\", tokenizer=None) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded training data\")\n",
        "    test_data, _ = load_data(\"test.txt\", tokenizer=tokenizer) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded test data\")\n",
        "\n",
        "    print('CBOW')\n",
        "    cbow_model = CBOWModel(tokenizer.get_vocab_size(), embedding_dim=100, context_size=3)\n",
        "    cbow_model.train(train_data, epochs=1)\n",
        "    print(tokenizer.decode(cbow_model.generate_text(test_data[0][:1])))\n",
        "    print(f\"Perplexity: {cbow_model.perplexity(test_data)}\")\n",
        "\n",
        "    # evaluate_models([ngram_model, log_linear_model,cbow_model], test_data)\n",
        "\n",
        "   \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "generative_ai_disabled": true,
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
